---
# permalink: /research/bias/
layout: post
title: Bias (coming soon)
sidenav: research
sticky_sidenav: true
---

All research is subject to bias, whether in our choice of who participates,  which data are collected, or how they’re interpreted. The following list is our starting point. Proactively engaging with bias helps us improve the credibility of our research.

### Research design bias

**What it is:** When the team doesn’t acknowledge bias or designs their research to advance existing beliefs. For example, if an agency executive believes that they already understand user needs, that executive might discourage the team from speaking directly to users—why learn what the executive already knows?


**How to counter:** Be mindful of the kinds of bias that can affect design research (which you're kind of already doing by reading this document). Clearly identify the team’s research objectives (which should center around [people’s needs](https://playbook.cio.gov/#play1)) and the research’s inherent limitations.


### Sampling bias

**What it is:** When some members of the target population are less likely to be included in the study. For example, if a team leans too heavily on [digital-first participant recruiting processes](https://18f.gsa.gov/2017/11/08/four-lessons-we-learned-while-building-our-own-design-research-recruiting-tool/), it risks excluding members of the public who don’t interact with government online.

**How to counter:** Clarify the target population, which can involve clarifying the difference between stakeholders (usually public servants) and users (usually the public), and look for ways to encourage diversity in the sample. Ask *Who haven’t we talked to yet (for example, people who access this service via screen reader)?* Discuss recruiting strategies, clarify the limitations of [snowball sampling](https://en.wikipedia.org/wiki/Snowball_sampling), and be careful of the conclusions drawn from any one study.  

### Interviewer bias

**What it is:** When the interviewer’s own beliefs or assumptions influence their inquiry. This can be especially apparent at the start of an interview—for example, if the interviewer expresses excitement about a particular aspect of the product or service (“The team is really proud of the new search feature!”).

**How to counter:** Be mindful of your ability to [prime](https://en.wikipedia.org/wiki/Priming_(psychology)) participants while building rapport, and in asking [close-ended questions](https://en.wikipedia.org/wiki/Closed-ended_question). Practice interviewing beforehand. Periodically reflect what you’ve heard during the interview back to interviewees (“just to be sure I heard you correctly, it sounds like you’re saying…”) and conduct [post-interview debriefs](https://methods.18f.gov/interview-debrief/). See [this blog post](https://www.juliemyoung.com/blog/2016/12/15/user-interviews-bias-and-how-to-reduce-it) for more information.

### Social desirability bias

**What it is:** The tendency for people to respond in ways that paint themselves in the best possible light, regardless of whether or not that bears any resemblance to reality. 

**How to counter:** Build rapport with participants. Emphasize the goals of the research, and how they’re better met with honest feedback. Distance yourself from any proposed design solutions (“these are just a few ideas the team came up with, but you’re the expert here”). Consider changing the research mode. For example, research by [Gibson](http://eprints.ncrm.ac.uk/1303/1/09-toolkit-email-interviews.pdf) has shown that social desirability bias may be less likely in online interviews, without a webcam, compared to face-to-face.


### Confirmation bias

**What it is:** When you (or your team) interprets research in a way that conforms with your own beliefs or values.

**How to counter:** Bring attention to the team’s shared values and beliefs—for example, by conducting a [hopes and fears exercise](https://www.iamnotmypixels.com/design-sprints-hopes-and-fears/) before the research begins. Invite the team to observe research in action, and hold [post-interview debriefs](https://methods.18f.gov/interview-debrief/). Carefully determine the point at which you’ve reached saturation. [Collaboratively analyze data before synthesizing it](https://18f.gsa.gov/2018/02/06/getting-partners-on-board-with-research-findings/), and note any plausible alternative interpretations.

### The observer effect (also known as the Hawthorne effect)

**What it is:** When the people who participate in research modify their behavior simply because they’re being observed. For example, if the office is unusually quiet while an interviewer is on site conducting interviews.

**How to counter:** Ask for introductions from key stakeholders, and be mindful of the ability to prime participants while informing their consent. Build rapport. Pay attention to what people say they do as well as what they actually do (ask participants to teach you their process). Use mixed methods, and unmoderated research modes, like monitoring forum posts or web analytics. Be careful to not over-interpret what you see or hear.

## Avoiding bias

We arrived at the preceding list through our own experience, and through close readings of Erika Hall’s *Just Enough Research* and Margaret R. Roller and Paul J. Lavrakas’s *Applied Qualitative Research Design.* Buster Benson’s [Cognitive bias cheat sheet](https://betterhumans.coach.me/cognitive-bias-cheat-sheet-55a472476b18) and [Cognitive Bias Codex](https://cdn-images-1.medium.com/max/2600/1*71TzKnr7bzXU_l_pU6DCNA.jpeg) provide more fulsome lists. 

In general, you can avoid bias and arrive at better solutions by intentionally including people throughout the design process. Help your team see [research as a team activity](/research/clarify-the-basics/), and understand why it’s better to talk to a few users throughout the design process than none at all (as Erika Hall says, “The most expensive [usability testing] of all is the kind your customers do for you after launch by way of customer service.”). 

Bias is a starting point for improving the team’s research practice—everyone benefits when we share a commitment to asking better questions.
