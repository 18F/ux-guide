---
permalink: /research/bias/
layout: post
title: Bias
sidenav: research
sticky_sidenav: true
---


All research is subject to bias, whether in our choice of who participates,  which data are collected, or how they’re interpreted. The following list is a starting point for bringing awareness to some of the biases that can shape research. Proactively engaging with them will improve the credibility of the research.

### Research design bias

Research design bias occurs when you either don’t acknowledge bias, or you design research to advance existing beliefs or to justify a predetermined course of action.

Counter research design bias by being mindful of the kinds of bias that can affect design research (which you're kind of already doing by reading this document). Identify the research objectives (which should center around [people’s needs](https://playbook.cio.gov/#play1)) and the inherent limitations of the research.

### Sampling bias

Sampling bias occurs when some of the people in your target population are less likely to be included in the study. For example, leaning too heavily on [digital-first participant recruiting processes](https://18f.gsa.gov/2017/11/08/four-lessons-we-learned-while-building-our-own-design-research-recruiting-tool/) risks excluding members of the public who don’t choose to interact with their government online.

To reduce sampling bias, explicitly discuss recruiting strategies while planning research, and be careful of the conclusions that you draw from any one study.  

### Interviewer bias

Interviewer bias occurs when the interviewer’s own beliefs or assumptions influence their inquiry. This can be especially apparent at the start of an interview; for example, if the interviewer were to express excitement about a particular aspect of the product or service (“the team is really proud of the new search feature!”).

Counter interviewer bias by practicing interviewing beforehand, and periodically reflect what you’ve heard during the interview back to interviewees (“just to be sure I heard you correctly, it sounds like you’re saying that...”). Conduct [moderated research critiques](https://18f.gsa.gov/2018/10/23/two-exercises-for-improving-design-research-through-reflective-practice/), [post-interview debriefs](https://methods.18f.gov/interview-debrief/), and [reflective journaling](https://docs.google.com/document/d/1abY0EJCE4zlPZYlDHak8khYxl-PcNKJW52JwxAd4k6o/edit).

### Social desirability bias

Everyone wants to be liked. Some people tend to respond in ways that paint themselves in the best possible light, regardless of whether or not that bears any resemblance to reality. Counter this by emphasizing with research participants the goals and benefits of the research, and build rapport. Also consider changing the research mode altogether. For example, research by [Gibson](http://eprints.ncrm.ac.uk/1303/1/09-toolkit-email-interviews.pdf) has shown that social desirability bias may be less likely in online interviews, without a webcam, compared to face-to-face.

### Confirmation bias

Confirmation bias occurs when you (and your team) *interpret* research in a way that conforms with your own beliefs or values.

Counter confirmation bias by bringing attention to your team's beliefs and values. Conduct a hopes and fears exercise before the research begins. Once the research is underway, invite stakeholders to observe it in action, and share insightful or incisive data (for example, strong opinions on the product or service in question) as they’re captured (for example, in Slack); continued exposure to research can help teams in separating emotional reactions from their rational interpretations. Engage observers in [post-interview debriefs](https://methods.18f.gov/interview-debrief/). After you’ve gathered data, [collaboratively analyze it before synthesizing it](https://18f.gsa.gov/2018/02/06/getting-partners-on-board-with-research-findings/). Be careful not to draw inferences until you’ve reached saturation, and note any plausible alternative interpretations.

### The Hawthorne effect

Sometimes the people who participate in our research modify their behavior simply because we’re observing. Focus more on what users do than what they say they do, and be careful to not over-interpret what you hear. Consider using unmoderated forms of research, like monitoring forum posts or web analytics. 

### References
1. *Just Enough Research* by Erika Hall
1. *Applied Qualitative Research Design: A Total Quality Framework Approach* by Margaret R. Roller and Paul J. Lavrakas
